\section{Experiment Design}
\subsection{Research Questions}
 To systematically investigate whether tuning can improve the 
 performance of baseline methods compared with deep learning method, we set
 the following three research questions:
 
 \bi
 \item RQ1: Can we reproduce Xu et al.'s baseline results (wordembedding+SVM)?
 \item RQ2: Is tuning SVM comparable with Xu et al.'s deep learning method in terms of performance scores?
 \item RQ3: Is tuning SVM comparable with Xu et al.'s deep learning method in terms of runtime?
 \ei
 
 RQ1 is to investigate whether our implementation of wordbedding +SVM method has
 the similar performance with Xu et al.'s baseline, which makes sure that our following 
 analysis can be generalized to Xu et al.'s conclusion. RQ2 and RQ3 lead us to
 investigate whether tuning SVM comparable with Xu et al.'s deep learning from both 
 performance and cost aspects.
 
 
\subsection{Dataset}
Our experimental data comes from Stack Overflow data dump of September 2016\footnote{https://archive.org/details/stackexchange},
where {\it posts} table include all the posts and answers posted on Stack Overflow up to date.
To insure that our results are closed to Xu et al.'s, we used the same training and testing
data as provided by the author\footnote{https://github.com/XBWer/ASEDataset}.

All the training/tuning/testing data used in this experiment are represented word embeddings.
To train the word2vec model, we randomly select 100,000 knowledge
 units(title, question body and all the answers) from {\it posts} table that are
 related to ``java''. 
 





\subsection{Evaluation Measures and Optimization Goals}
Recall from Algorithm~1 that we call differential evolution once for each
 optimization goal. This section lists those optimization goals.
Let $\{A,B,C,D\}$ denote the
true negatives, 
false negatives, 
false positives, and 
true positives
(respectively) found by a binary detector. 
Certain standard measures can be computed from
$A,B,C,D$, as shown below. Note that for $pf$, the {\em better} scores are {\em smaller}
while
for all other scores, the {\em better} scores are {\em larger}.

{\scriptsize\[
\begin{array}{ll}
pd=recall=&D/(B+D)\\
pf=&C/(A+C)\\ 
prec=precision=&D/(D+C) \\
F =&2*pd*prec/(pd + prec)
\end{array}
\]}

The rest of this paper explores tuning for {\em prec} and {\em F}. As discussed
in \tion{goals}, our point is not that these are best or most important optimization goals.
Indeed, the list of ``most important'' goals is domain-specific (see \tion{goals})
and we only explore these two to illustrate how conclusions can change dramatically
when moving from one goal to another.




\subsection{RQ1}
\subsection{RQ2}
