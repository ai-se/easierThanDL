\documentclass[sigconf,review, anonymous]{acmart}
\acmConference[ESEC/FSE 2017]{Submitted to 11th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering}{4--8 September, 2017}{Paderborn, Germany}

\usepackage{booktabs} % For formal tables
\usepackage{algorithm, algorithmicx} % pseudo code
\usepackage{algpseudocode}% pseudo code
\usepackage{multirow}
\usepackage{tabu}
\usepackage{enumitem} % handle itemize, enumerate 


\captionsetup{belowskip=2pt,aboveskip=2pt} % space between caption and content



%##############Another result box
\usepackage[framemethod=tikz]{mdframed}
\usepackage{lipsum}
\usetikzlibrary{shadows}
\usepackage{mathtools}
\usepackage{graphics}
\newmdenv[
tikzsetting= {fill=white},
linewidth=1pt,
roundcorner=2pt, 
shadow=false
]{myshadowbox}

%########## Result Box##########
\usepackage{color,colortbl}
\usepackage{xcolor}
\definecolor{lightgray}{gray}{0.8}

\makeatletter
\let\th@plain\relax
\makeatother

\usepackage[framed]{ntheorem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shadows}

\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\theoremclass{Lesson}
\theoremstyle{break}
\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black,
 fill=Gray,  drop shadow={fill=black, opacity=1}]
\newcommand\thmbox[1]{%
 \noindent\begin{tikzpicture}%
 \node [thmbox] (box){%
\begin{minipage}{.94\textwidth}%    
     \vspace{-1mm}#1\vspace{-1mm}%
   \end{minipage}%
 };%
 \end{tikzpicture}}
 

\newcommand{\wei}[1]{\textcolor{red}{Wei: #1}} 
\newcommand{\Menzies}[1]{\textcolor{red}{Dr.Menzies: #1}} 
 




%% timm tricks
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}

\let\theoremframecommand\thmbox
\newshadedtheorem{lesson}{Result}



% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
% \acmDOI{10.475/123_4}

% ISBN
% \acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[FSE'2017]{ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING}{September 2017}{PADERBORN, GERMANY} 
\acmYear{2017}
\copyrightyear{2017}

\acmPrice{15.00}


\begin{document}
\title{Think Simple Methods First: a Deep Learning Case Study}
% \titlenote{Produces the permission block, and
%   copyright information}
% \subtitle{Extended Abstract}
% \subtitlenote{The full version of the author's guide is available as
%   \texttt{acmart.pdf} document}


\author{Wei Fu}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Computer Science Department, North Carolina State University}
  \streetaddress{890 Oval Drive}
  \city{Raleigh} 
  \state{North Carolina} 
  \postcode{27606}
}
\email{wfu@ncsu.edu}

\author{Tim Menzies}
\affiliation{%
  \institution{Computer Science Department, North Carolina State University}
  \streetaddress{890 Oval Drive}
  \city{Raleigh} 
  \state{North Carolina} 
  \postcode{27606}
}
\email{tim.menzies@gmail.com}



\begin{abstract}
TBD!
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>  
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

% We no longer use \terms command
%\terms{Theory}

\keywords{software analytics, parameter tuning, deep learning, SVM, CNN, linkable knowledge units prediction.}


\maketitle

% \pagestyle{headings}
% \setcounter{page}{1}
%\pagenumbering{arabic} %XXX delete before submission
% \input{samplebody-conf}


\section{Introduction}
\section{Background and Related Work}

\subsection{Deep Learning}

\subsection{Deep Learning in SE}
With the vast amounts of computational power and data, 
deep learning has been proven to be a very powerful method 
by researchers in many fields\cite{lecun2015deep}, like computer vision and natural language processing\cite{krizhevsky2012imagenet,mikolov2013distributed,sutskever2014sequence}. 
Recently, it also has attracted  attentions from researchers and practitioners in software
 community\cite{wang2016automatically, gu2016deep, xu2016predicting,white2016deep,white2015toward,lam2015combining,choetkiertikul2016deep}.
 These researchers applied  deep learning techniques to solve various problems,
 including defect prediction, bug localization, clone code detection, API recommendation, 
 effort estimation and linkable knowledge prediction.
 
By carefully reading, these work can be divided into the following two categories:
 
\begin{itemize}
\item treat deep learning as a feature extractor, and then apply other regular machine learning to do further job.
\item apply deep learning directly to solve the problems.
\end{itemize}

Lam et al.~\cite{lam2015combining}  proposed an approach to apply deep neural network
 in combination with rVSM to automatically locate the potential buggy files for a given
 bug report. By comparing it to the baseline methods(learn-to-rank\cite{ye2014learning}, 
 BugLocator~\cite{zhou2012should}), authors reported $8-20.8\%$  and $2.7-20.7\%$ 
 higher top-1 accuracy than baseline methods, respectively. The training time for deep neural
 network was reported from 70 to 122 minutes on a computer with 32 cores CPU,
 126 GB RAM machine. However,
 no such time information reported about baseline methods.
 
 Wang et al.~\cite{wang2016automatically} applied deep belief network to automatically
 learn semantic features from token vectors extracted from the studied program. After
 that, Naive Bayes and Logistic Regression methods are used to evaluate the effectiveness
 of such feature generation method as well as PROMISE and AST features. In terms of
 running time, Wang et al. only reported time for generating semantics features with deep belief network, which
 ranged from 8 seconds to 32 seconds. However, the time for training and tuning deep belief network is
 missing. Furthermore, to evaluate the effectiveness of deep belief network in terms of time cost, 
 it would be favorable to include all the time spent on feature extraction, including
 paring source code, token generation.
 
 Choetkiertikul et al.~\cite{choetkiertikul2016deep} proposed to apply deep learning techniques
 to solve effort estimation problem, where they used long short-term memory(LSTM) to learn
 feature vectors from the title, description and comments associated with an issue report and
 regular machine learning techniques applied afterwards. LSTM was reported to have a 
 significant improvement over the baseline method bag-of-word. No further information regarding
 runtime was reported for both methods.
 
 White et al.~\cite{white2015toward, white2016deep} applied
 recurrent neural networks, one type of  deep learning techniques, 
 to address code clone detection and code suggestion. As they reported,
 the average training time for 8 projects were ranging from 34 seconds
  to 2977 seconds for each epoch on a two 3.3 GHz
 CPUs computer and each project required at least 30 epochs~\cite{white2016deep}.
 For the {\it JDK} project in their experiment, it would take 25 hours 
 on the same computer to train the models before getting prediction.
 For the time cost for code suggestions, authors didn't mention any related information~\cite{white2015toward}.

Gu et al.~\cite{gu2016deep} proposed  a recurrent neural network(RNN)
 based method, D{\scriptsize EEP}API, to generate API usage sequences for a given natural language query. 
 Compared with the baseline method {\it SWIM}~\cite{raghothaman2016swim} and 
 {\it Lucene + UP-Miner}~\cite{wang2013mining},  D{\scriptsize EEP}API has improved the performance a lot.
 However, one can not ignore the fact that such model was trained with a Nivdia K20 GPU for about 240 hours.
 
 Xu et al.~\cite{xu2016predicting} utilized neural language model and  
 convolutional neural network(CNN) to  learn word-level and document-level features to
 predict semantically linkable knowledge units in Stack Overflow. 
 In terms of performance metrics, like precision, recall and F1-score,
 CNN method was evaluated much better than 
 the baseline method support vector machine(SVM). 
 However, the time cost for training CNN is not ignorable as it took
 14 hours to train CNN model on a 2.5GHz PC with 16 GB RAM 
 to achieve the relative low loss convergence.
 
 In summary, all the above work authors are trying hard to promote deep learning in software
 engineering community. They presented somewhat or even much better results compared with
 the baseline methods. However, they either didn't present the computational and time cost, like \cite{white2016deep,white2015toward,lam2015combining,choetkiertikul2016deep}, or simply listed
 the cost as it is without further discussion\cite{wang2016automatically, gu2016deep, xu2016predicting}. Without comparing cost with the baseline methods like all above paper,
 we actually don't have a concrete idea about how well deep learning can solve software engineering problems in terms of both benefits and costs. Especially when we don't have much knowledge about
 whether such problems are suitable for deep learning. 
 
As deep learning techniques cost huge amount of time and computational
resources to train its model,
one might ask question whether the improvements from deep learning is worth
the costs. {\it Are there any simple techniques that achieve similar improvements
with less resource costs?} and {\it what might be the baseline methods to compare with
when deep learning is applied?}

In this paper, we revisit the research problems 
from Xu et al.\cite{xu2016predicting} work, and by applying parameter tuning 
to SVM algorithms, we find that the results we've got, on average, are even 
better than their CNN results with quite less training and tuning time.
Specifically, SVM with optimal tunings have won over deep learning 
in $\frac{8}{12}$ performance scores and for those $\frac{4}{12}$,
tuned SVM  does not lose much. However, CNN took 14 hours to achieve
those scores and parameter tuning only require 10 minutes, which is almost 80X faster.


\subsection{Parameter Tuning in SE}
Machine learning algorithms are designed to explore the instances
to learn the bias. However, most of these algorithms are controlled by parameters
, like the depth of the tree in CART, which would change the
exploration behavior if set different tunings. This parameter(or Hyper-parameter)
tuning problem is well explored in other community \cite{bergstra2012random,li2016hyperband}.
In software engineering community, this issue is ignored for quite
long time and recently, there's been a trend to starting investigation on such effect.

Fu et al.\cite{fu2016tuning} surveyed hundreds of highly 
cited software engineering paper in area of defect prediction. 
Their observation is that most software engineering  researchers
didn't acknowledged the impact of tunings 
(exceptions: \cite{lessmann2008benchmarking,tantithamthavorn2016automated}) and
use the ``off-the-shelf''data miners. For example, 
Elish et al.\cite{elish2008predicting} compared support vector machines
to other data miners for the purposes of defect prediction.
However, the Elish et al. paper makes no mention of any SVM tuning study.
For those two exceptions\cite{lessmann2008benchmarking,tantithamthavorn2016automated}, 
they simply apply grid search to explore the potential parameter space for optimial tunings.
However, Bergstra et al.\cite{bergstra2012random} and 
Fu et al.\cite{fu2016differential} argue that random search and 
differential evolution(DE) algorithm are better than 
grid search in terms of efficiency and performance.
Fu et al showed that finding useful tunings for software defection is remarkably easy and fast using DE.

Recently, Argrawa et al.\cite{agrawal2016wrong} investigated 
impact of parameter tuning on Latent Dirichlet Allocation(LDA),
which is a widely used technique in software engineering field
to find related topics within unstructured text, 
like topics analytics in stack overflow \cite{barua2014developers}
and source code analysis \cite{binkley2014understanding}.
According to Argrawa et al., LDA suffers from the instability issue.
Parameters within LDA are one reason to cause such problem. However,
quite few researchers(4 out of 57 papers) using LDA worried about
that tuning might have a large impact on the results. By applying DE to find 
optimal tunings,  Argrawa et al. find that the resultant LDA is able to generate more stable results
for both supervised learning and unsupervised learning.
They also confirm the statement made by Fu et al.\cite{fu2016tuning} that
parameter tuning with DE is not extremely slow and DE is easy to implement.

In this work, parameter tuning is applied to the baseline method for Deep Learning
to better understand how deep learning perform in terms of efficiency and performance.
Since (a)tuning with DE is easy to implement and fast to run; (b) it is shown to be able
to improve learners' performance, it will not introduce much overhead to the baseline method.

To our best knowledge, this is the first paper
applying parameter tuning to the baseline method for Deep learning methods in 
software engineering community. Our results show that no further software analytics
paper with Deep Learning just use ``off-the-shelf'' data miners as the baseline
method to compare with. Furthermore, since Deep Learning is such a resource consuming
technique and software analytics tasks are not extremely complicated, the cost for 
both baseline methods and Deep learning method should be supplied.



\section{Method}

\subsection{Reserach problem}\label{problem}
To investigate {\it how deep learning techniques perform on software analytics compared 
with tuning baseline method}, we pick Xu et al.~\cite{xu2016predicting} work as a case study
where convolutional neural network(CNN), a deep learning technique, was proposed to 
solve a multi-class classification problem based on text data from Stack Overflow.

In that work, Xu et al. predict whether two questions posted on Stack Overflow are semantically linkable. 
Specifically,  a question along with its entire set of answers posted in Stack Overflow
as a {\it knowledge unit}. If two knowledge units are semantically related, they're considered
as {\it linkable} knowledge units. To predict relationships between two questions more precisely, 
Xu et al. further divide linkable  units 
into {\it duplicate}, {\it direct link}, {\it indirect link} and {\it isolated}  four different categories 
based on its relatedness. The details are listed as follows~\cite{xu2016predicting}:

\begin{itemize}
\item Duplicate:  these two knowledge units are addressing the same question.
\item Direct link: one knowledge unit can help to answer the question in the other knowledge unit.
\item Indirect link: one knowledge provide similar information to solve the question in the other knowledge unit, but not a direct answer.
\item Isolated: these two knowledge units discuss unrelated questions.
\end{itemize}

In that paper, Xu et al. provided the following two methods as baselines:

\begin{itemize}
\item TF-IDF + SVM: a multi-class SVM classifier with  36 textual features generated  based on the 
TF and IDF values of the words in a pair of knowledge units. 
\item Word Embedding + SVM:  a multi-class SVM classifier with word embedding generate by word2vec model~\cite{mikolov2013distributed}.
\end{itemize}
Both of these two baseline methods are compared against the proposed method, word embedding + CNN. 

Our concern is whether parameter tuning could help improve baseline method performance.
In this work, to fairly compare deep learning method to baseline methods with the parameter tuning,
we choose word embedding + SVM as our research subject since it uses the word embedding as the
input data, which is the same as CNN method. 
Instead of setting SVM parameters to constant values, we apply DE as a parameter tuner to find optimal tunings
for SVM. Then we compare the performance score of tuned SVM  with the CNN scores reported by Xu et al. 

%%%%%%%%%%%%%%%% list of parameters%%%%%%%%%%%%%%%%%%%%%
%\renewcommand\arraystretch{1.2}
\begin{table*}[htp]
%\scriptsize
   \caption {List of Parameters Tuned by This Paper.}
\centering
\resizebox{\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|l|}
	\cline{1-5}
	Parameters & Default &Xue et al.&Tuning Range& 
\multicolumn{1}{c|}{Description} \\ \hline
	C & 1.0 &unkown&[1, 50]& Penalty parameter C of the error term.\\ \cline{1-5} 
	 kernel & `rbf' &`rbf'&[`liner',`poly',`rbf',`sigmoid']& Specify the kernel type to be used in the algorithms. \\ \cline{1-5} 
	 gamma & {1/n\_features} &$1/200$& [0, 1]& Kernel coefficient for `rbf', `poly' and `sigmoid'. \\ \cline{1-5} 
	 coef0 & 0 & unkown & [0, 1] &  Independent term in kernel function. It is only used in `poly' and `sigmoid'. \\ \cline{1-5}
\hline
	\end{tabular}}
\label{tab:parameters}
\end{table*}

\subsection{Learners and their parameters}
SVM has been proven to be a very successful method to solve
text classification problem. It seeks to minimize misclassification
errors by selecting a boundary or hyperplane that leaves
the maximum margin between positive and negative classes, where the
margin is defined as the sum of the distances of the
hyperplane from the closest point of the two classes\cite{joachims1998text}.

Like most machine learning algorithms, there are some parameters associated with
SMV to control how it learns.  In Xu et al.'s experiment, they used RBF for SVM kernel
and set $\gamma$ to $1/k$, where $k$ is $36$ for TF-IDF + SVM method
and $200$ for Word Embedding + SVM method. For other parameters, 
Xu et al. mentioned that grid search was applied to optimize the SVM parameters, 
but no further information disclose. 

In this work, we use the SVM module from Scikit-learn~\cite{scikit-learn}, a python package for machine learning,
where the following parameters shown in Table. ~\ref{tab:parameters} are selected for tuning.
Parameter {\it C} is to set the amount of regularization, which controls the tradeoff between
the errors on training data and the model complexity.  A small value for {\it C} will generate 
a simple model with more training errors, while a large value will lead to a complicated model with fewer
errors. {\it Kernel} is to introduce different nonlinearities into the SVM model by applying kernel functions
on the input data. {\it Gamma } defines how far the influence of a single training example reaches, 
with low values meaning `far' and high values meaning `close'. {\it coef0} is an independent parameter used
in sigmod and  polynomial kernel function.

As to why we used the ``Tuning Range'' shown in {parameters}, and not some other ranges,
we note that (1)~those ranges included the defaults and also Xu et al.'s values ; (2)~the results shown below
show that by exploring those ranges,  we achieved large gains in the performance of our baseline method.
This is not to say that {\em larger} tuning ranges might not result in {\em greater} improvements.
However, for the goals of this paper (to show that tuning baseline method do matter), exploring
just these ranges shown in \tab{parameters} will suffice.


%In this work, we investigate whether parameter tunings can improve the baseline method based on Xu et al. work.
%Specifically, to compare with the proposed method Word Embedding + CNN, we choose Word Embedding + SVM
%as our baseline, where both CNN and SVM methods are supplied with the same word embedding feature data. 


\subsection{Learning Word Embedding}
Learning word embeddings refers to finding vector representations
of words such that the similarities between words can be captured by cosine similarity of corresponding 
vector representations. It is been shown that the words with similar semantic and syntactic are found closed
to each other in the embedding space~\cite{mikolov2013distributed}.

Several methods have been proposed to generate word embeddings, 
like skip-gram~\cite{mikolov2013distributed}, GloVe ~\cite{pennington2014glove}
and PCA on the word co-occurrence matrix~\cite{lebret2013word}. To replicate Xu et al. work,
we used continuous skip-gram model(word2vec),  which is a unsupervised word representation learning method based on
neural networks. 

The skip-gram model learns vector representations of words
 by predicting the surrounding words in a context window. 
 Given a sentence of words $W =w_1$,$w_2$,...,$w_n$, the objective of skip-gram model is to maximize the
 the average log probability of the surrounding words:
 \begin{equation*}
 \frac{1}{n}\sum_{i=1}^{n} \sum_{-c\leq j \leq c, j \neq 0} log p(w_{i+j}|w_i)
\end{equation*}
where $c$ is the context window size and $w_{t+j}$ and $w_{t}$ represent surrounding words and center word, respectively.
The probability of $p(w_{i+j}|w_i)$ is computed according to the softmax function:

\begin{equation*}
p(w_O|w_I) = \frac{exp(v_{w_O}^Tv_{w_I})}{\sum_{w=1}^{|W|}exp(v_{w}^Tv_{w_I})}
\end{equation*}
where $v_{w_I}$ and $v_{w_O}$ are the vector representations of the input and output vectors of $w$, respectively. 
$\sum_{w=1}^{|W|}exp(v_{w}^Tv_{w_I})$  normalizes the inner product results across all the words.
To improve the computation efficiency, Mikolove et al. \cite{mikolov2013distributed} proposed
hierachical softmax and negative sampling
techniques. More details can be found in ~\cite{mikolov2013distributed}.

As noted, skip-gram itself has several parameters that will drive the algorithm 
to learn the word embeddings,  like {\it window size} and {\it dimensionality of embedding space}. 
Zucoon et al. \cite{zuccon2015integrating} found that embedding dimensionality
and context window size have no consistent impact on retrieval model performance. However,
Yang et al.~\cite{yang2016using} showed that large context window and dimension
 sizes are preferable to improve the performance when using CNN to solve  classification tasks
 for Twitter. Since this work is to compare performance of  tuning SVM  with CNNs, where
 skip-gram model is used to generate word vector representations for both of thesemethods, 
 tuning parameter of skip-gram model is beyond the scope of this paper and we will leave it to feature work.
 
 

To train our word2vec model, $100,000$ knowledge units tagged with ``java'' from
Stack Overflow {\it posts} table  (include titles, questions and answers)
are randomly selected as a word corpus\footnote{Without further explanation, 
all the experiment settings, including learner algorithms,
training/testing data split, etc, strictly follow Xu et al.'s work. }. 
After applying proper data processing techniques proposed by Xu et al., like
 remove the unnecessary HTML tags and keep short code snippets in
{\it code} tag, then fit the corpus into {\it gensim} word2vec module ~\cite{rehurek_lrec},
which is a python wrapper over original word2vec package.

When converting knowledge units into vector representations, 
for each word $w_i$ in the post processed knowledge unit(including title, question and answers),
we query the trained skip-gram model to get the corresponding word vector representation $v_i$.
Then the whole knowledge unit with $s$ words
will be converted to vector representation by element-wise addition, $Uv = v_i \oplus v_2 \oplus...\oplus v_s $. 
This vector representation will be used
as the input data to SVM.




\subsection{Tuning Algorithm}

Tuning algorithm is an optimizer that will drive the learner to explore
the optimal parameter in a given searching space. According to our
literature review, there are several searching algorithms used in 
SE community:{\em 
simulated annealing}~\cite{feather2002converging,menzies2007data};
 various {\em genetic algorithms}~\cite{jones1996automatic,harman2007current, arcuri2011parameter} augmented by
techniques such as {\em differential evolution}
~\cite{storn1997differential, fu2016tuning, fu2016differential,chaves2015differential,agrawal2016wrong}, 
{\em tabu search} and {\em scatter search}~\cite{beausoleil2006moss,molina2007sspmo,corazza2013using};
{\em particle swarm optimization}~\cite{windisch2007applying}; 
numerous {\em decomposition} approaches that use
    heuristics to decompose the total space into   small problems,   then apply a
    {\em response surface methods}~\cite{krall2015gale};
     {\em NSGA II} ~\cite{zhang2007multi}and {\em NSGA III}~\cite{mkaouer2014high}.


Of all the mentioned algorithms,  the simplest are simulated annealing (SA)  and 
differential evolution (DE), each of which can be coded in less than a page of some high-level scripting language.
 Our reading of the current literature is that there are more  advocates for
differential evolution than SA. For example,  Vesterstrom and Thomsen~\cite{Vesterstrom04} found DE to be competitive with 
 particle swarm optimization and other GAs.  DEs have already been applied before for 
 parameter tuning (e.g. see~\cite{omran2005differential, chiha2012tuning, fu2016tuning, fu2016differential, agrawal2016wrong}) .
Therefore, in this work, we adopt DE as our tuning algorithm and 
the pseudocode for DE is shown in Algorithm~\ref{alg:DE}.
To better explain how DE work, in the following description, 
line number in the pseudocode is denoted as superscript numbers.
\wei{need to replace the pesudocode to the simpler one!}

\begin{algorithm}[!htp]

\scriptsize
\begin{algorithmic}[1]
\Require $\mathit{np} = 10$, $f=0.75$, $cr=0.3$, $\mathit{life} = 5$, $\mathit{Goal} \in \{\mathit{pd},f,...\}$
\Ensure $\mathit{S_{best}}$
\vspace{2mm}
\Function{DE}{$\mathit{np}$, $f$, $cr$, $\mathit{life}$, $\mathit{Goal}$}
 \State $\mathit{Population}  \gets $ InitializePopulation($\mathit{np}$)   
 \State $\mathit{S_{best}} \gets $GetBestSolution($\mathit{Population}$)
 \While{$\mathit{life} > 0$}
    \State $NewGeneration \gets \emptyset$
    \For{$i=0 \to \mathit{np}-1$}
        \State $\mathit{S_i} \gets$ Extrapolate($\mathit{Population [i], Population , cr, f}$)
        \If {Score($S_i$) $>$Score($\mathit{Population [i]}$)}
            \State $\mathit{NewGeneration}$.append($\mathit{S_i}$)
        \Else
        \State $\mathit{NewGeneration}$.append($\mathit{Population [i]}$)
        \EndIf
    \EndFor
    \State $\mathit{Population  \gets NewGeneration}$
    \If{$\neg$ Improve($\mathit{Population} $)}
        \State $\mathit{life -=1}$
    \EndIf
    \State $\mathit{S_{best} }\gets$ GetBestSolution($\mathit{Population} $)
 \EndWhile
\State \Return $\mathit{S_{best}}$
\EndFunction
\Function{Score}{$\mathit{Candidate}$}
  \State set tuned parameters according to $\mathit{Candidate}$
  \State $\mathit{model} \gets$TrainLearner()
  \State $\mathit{result} \gets$TestLearner($\mathit{model}$)   
  \State \Return$\mathit{Goal}(\mathit{result})$  
\EndFunction
\Function{Extrapolate}{$\mathit{old, pop, cr, f}$}
  \State $\mathit{a, b, c}\gets threeOthers(\mathit{pop,old})$  
  \State $\mathit{newf }\gets \emptyset$
  \For{$i=0 \to \mathit{np}-1$}
        \If{$\mathit{cr < random()}$}
            \State $\mathit{newf}$.append($\mathit{old[i]}$)
        \Else
            \If{typeof($\mathit{old[i]}$) == bool}
                \State $\mathit{newf}$.append(not $\mathit{old[i]}$)
            \Else
                \State $\mathit{newf}$.append(trim($i$, ($\mathit{a[i]+f*(b[i]-c[i]}$)))) 
         \EndIf
      \EndIf
  \EndFor
 \State \Return $\mathit{newf}$
\EndFunction
\end{algorithmic} 
\caption{Pseudocode for DE with Early Termination}
\label{alg:DE}
\end{algorithm}



%   \begin{figure}[!t]\small
%      \begin{tabular}{|p{.95\linewidth}|}\hline
%       1. Given a model with $n$ decisions,
%       OPTIMIZE calls SAMPLE $N=10*n$ times.
%       Each call generates one member of the population {\em pop$_{i\in N}$}.

%       2. OPTIMIZE scores each {\em pop}$_i$ according to various objective
%       scores $o$. In the case of our goal models, the objectives are $o_1$ the sum of the cost
%      of its decisions, $o_2$ the number of ignore edges, and the number of $o_3$ satisfied goals
%      and $o_4$  softgoals.

%      3. OPTIMIZE tries to each replace {\em pop}$_i$ with a mutant $q$
%      built by extrapolating between three other members of population $a,b,c$.
%      At probability $p_1$, for each decision $a_k \in a$, then
%      $m_k= a_k \vee (p_1 < \mathit{rand}() \wedge( b_k \vee c_k))$.

%      4. Each mutant $m$ is assessed by calling  $\text{SAMPLE}(\textit{model,prior=m})$;
%      i.e. by seeing what can be achieved within a goal after first assuming
%      that $\textit{prior}=m$.

%      5.  To test if the mutant $m$ is preferred to {\em pop}$_i$, OPTIMIZE uses
%       Zitler's continuous domination {\em cdom}
%       predicate~\cite{Zitzler2004}. This predicate compares two sets of objectives
%       from sets $x$ and $y$. In that comparison,
%       $x$ is better than another $y$ if $x$  ``losses'' least.
%       In the following, $``n''$ is the number of objectives and $w_j \in \{-1, 1\}$
% shows if we seek to maximize $o_j$.
% \[
% \begin{array}{rcl}
% x \succ y & =& \textit{loss}(y,x) > \textit{loss}(x,y)\\
% \textit{loss}(x,y)& = &\sum_j^n -e^{\Delta(j,x,y,n)}/n\\
% \Delta(j,x,y,n) & = & w_j(o_{j,x}  - o_{j,y})/n
% \end{array}
% \]

% 5. OPTIMIZE repeatedly loops over the population, trying to replace  items with mutants,
% until new better mutants stop being found.

% 6. Return the population.
% \\\hline
% \end{tabular}
%      \caption{Procedure OPTIMIZE: strives to find ``good'' priors which,
%       when passes to SAMPLE, maximize the number of edges used
%       while also minimizing cost, and
%   maximizing satisfied hard goals and soft goals.
%   OPTIMIZE is based on Storn's differential evolution optimizer~\protect\cite{storn1997differential}.
%   OPTIMIZE is called by the RANK procedure of \fig{rank}.
%   For the reader unfamiliar with the mutation technique of step 3 and the {\em cdom}
%   scoring of step~5, we note that these
%   are standard practice in the search-based
%   SE community~\cite{Fu2016,krall2015gale}.
% }\label{fig:optimize}
% \end{figure}

DE evolves a {\em NewGeneration} of candidates  from
a current {\em Population}.  Our DE's lose one ``life''
when the new population is no better than  current one (terminating when ``life'' is zero)$^{L4}$.
Each candidate solution in the {\em Population}  
is a pair of {\em (Tunings, Scores)}.  {\em Tunings} are selected from
{parameters} and {\em Scores} come from training a learner using those parameters
and applying it     test data$^{L23-L27}$.

The premise of DE  is that the best way to mutate the existing tunings
is to {\em Extrapolate}$^{L28}$
between current solutions.  Three solutions $a,b,c$ are selected at random.
For each tuning parameter $i$, at some probability {\em cr}, we replace
the old tuning $x_i$ with $y_i$. For booleans, we use $y_i= \neg x_i$ (see line 36). For numerics, $y_i = a_i+f \times (b_i - c_i)$   where $f$ is a parameter
controlling  cross-over.  The {\em trim} function$^{L38}$ limits the new
value to the legal range min..max of that parameter.
 
The main loop of DE$^{L6}$ runs over the {\em Population}, replacing old items
with new {\em Candidate}s (if  new candidate is better).
This means that, as the loop progresses, the {\em Population} is full of increasingly
more valuable solutions. This, in turn, also improves  the candidates, which are {\em Extrapolate}d
from the {\em Population}.

For the experiments of this paper, we collect performance
values from SVM, from which a {\em Goal} function extracts one 
performance value$^{L26}$ (so we run this code many times, each time with
a different {\em Goal}$^{L1}$).  Technically, this makes a  {\em single objective} DE 
(and for notes on multi-objective DEs, see~\cite{robivc2005demo,zhang2007moea,huang2010differential}).


\section{Experiment Design}
\subsection{Research Questions}\label{RQ}
 To systematically investigate whether tuning can improve the 
 performance of baseline methods compared with deep learning method, we set
 the following three research questions:
 
 \bi
 \item RQ1: Can we reproduce Xu et al.'s baseline results (wordembedding+SVM)?
 \item RQ2: Is tuning SVM comparable with Xu et al.'s deep learning method in terms of performance scores?
 \item RQ3: Is tuning SVM running faster than Xu et al.'s deep learning method in terms of runtime?
 \ei
 
 RQ1 is to investigate whether our implementation of wordbedding +SVM method has
 the similar performance with Xu et al.'s baseline, which makes sure that our following 
 analysis can be generalized to Xu et al.'s conclusion. RQ2 and RQ3 lead us to
 investigate whether tuning SVM comparable with Xu et al.'s deep learning from both 
 performance and cost aspects.
 
 \begin{figure*}
    \centering
%    \includegraphics[width=0.48\textwidth,height=2.7in]{pic/workflow.pdf} % THIS IS FOR SINGLE COLUM
     \includegraphics[width=\textwidth,height=3.3in]{pic/workflow.pdf} % THIS IS FOR SINGLE COLUM
    \caption{The Overall Workflow of Building Knowledge Units Predictor with Tuned SVM}
    \label{fig:workflow}
\end{figure*}

\subsection{Dataset and Experimental Settings}
Our experimental data comes from Stack Overflow data dump of 
September 2016\footnote{https://archive.org/details/stackexchange},
where {\it posts} table includes all the questions and answers posted on Stack Overflow
up to date and the {\it postlinks} table describes the relationships between posts, 
e.g., {\it duplicate} and {\it linked}. As mentioned in Section
\ref{problem}, we have four different types of relationships in knowledge units.
Therefore,  {\it linked} type is further divided into {\it indirectly linked} and {\it directly linked}.
Overall, four different types of data are generated according the following rules:
\bi
\item Randomly select a pair of posts from the {\it postlinks} table, if the value
in  {\it PostLinkTypeId} field for this pair of posts is $3$, then this pair of posts is {\it duplicate} posts. 
Otherwise they're {\it directly linked} posts.

\item Randomly select a pair of posts from the {\it posts} table, if this pair of posts is linkable from each other according to
{\it postlinks} table and the distance between them are greater than 2, then this pair of posts is indirectly linked. If they're
not linkable, then this pair of posts is {isolated}.
\ei

In this work, we use the same training and testing
knowledge pairs as in~\cite{xu2016predicting}\footnote{https://github.com/XBWer/ASEDataset}, 
where 6,400 pairs of  knowledge units for training and 1,600 pairs for testing. And each type 
of linked knowledge units accounts for $1/4$ in both training and testing data. The reasons are:
\bi
\item It is to insure that  performance of our baseline method are as closed to Xu et al.'s as possible.
\item Since deep learning method is way complicate compared with SVM and a little difference in implementations
might lead to different results, to fairly compare with Xu et al's result, we simply use the  performance scores
of CNN method from~\cite{xu2016predicting} without any implementation bias introduced.
\ei

For training word2vec model, we randomly select 100,000 knowledge
 units(title, question body and all the answers) from {\it posts} table that are
 related to ``java''. After that, all the training/tuning/testing knowledge units
 used in this paper are converted into word embedding representations by looking up
 each word in wrod2vec model.
 
As seen in \fig{workflow}, instead of using all the 6,400 knowledge units as training data, 
we split the original training data into {\it new training data} and {\it tuning data}, which are
used during parameter tuning procedure for training SVM and evaluating candidate
parameters offered by DE. Afterwards, the {\it new training} data is again fitted into the SVM
with the optimal parameters found by DE and finally  the performance of the tuned
SVM will be evaluated on the  {\it testing data}.

To reduce the potential variance caused
by how the original training data is divided, {\it 10-fold cross-validation} is performed, where
each time one fold with $640$ knowledge units paris as the tuning data, and the remaining folds with $5760$
knowledge units as  the new training data, then the output SVM model will be evaluated on the testing data. Therefore,
all the performance scores reported below are averaged values over 10 runs.



\subsection{Evaluation Metrics}
When evaluating the performance of tuning SVM on the
multi-class linkable knowledge units prediction problem,
consistent with Xu et al.\cite{xu2016predicting}, we use accuracy, precision, recall and F1-score
as the evaluation metics.

\begin{table}[htp]
\caption {Confusion Matrix.}
\scriptsize
\resizebox{0.38\textwidth}{!}{
\begin{tabular} {@{}cc|c|c|c|c|l@{}}
\cline{3-6}
& & \multicolumn{4}{ c| }{Classified as} \\ \cline{3-6}
& & $C_1$ & $C_2$ & $C_3$ & $C_4$ \\ \cline{1-6}
\multicolumn{1}{ |c  }{\multirow{4}{*}{Actual} } &
\multicolumn{1}{ |c|| }{$C_1$} & $c_{11}$ & $c_{12}$  & $c_{13}$ & $c_{14}$  & \\ \cline{2-6}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |c|| }{$C_2$} & $c_{21}$& $c_{22}$ & $c_{23}$ & $c_{24}$ &  \\ \cline{2-6}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |c|| }{$C_3$} & $c_{31}$ & $c_{32}$ & $c_{33}$ & $c_{34}$ & \\ \cline{2-6}
\multicolumn{1}{ |c  }{}                        &
\multicolumn{1}{ |c|| }{$C_4$} & $c_{41}$ & $c_{42}$ & $c_{43}$ & $c_{44}$ & \\ \cline{1-6}
\end{tabular}}

\label{tab:confusion}
\end{table}

Given a multi-classification problem with true labels $C_1$, 
$C_2$, $C_3$ and $C_4$, we can generate a confusion matrix like \tab{confusion}, 
where the value of $c_{ii}$ represents the number of instances that are correctly classified
by the learner for class $C_i$. 

{\it Accuracy} of the learner is defined as the number of  correctly
classified knowledge units over the total number of knowledge units, i.e.,


{\[
\begin{array}{ll}
accuracy = \frac{\sum_i c_{ii}}{\sum_{i}\sum_{j}c_{ij}}
\end{array}
\]}
Where ${\sum_{i}\sum_{j}c_{ij}}$ is the total number of knowledge units.
For a given type of knowledge units, $C_j$, the {\it precision} is defined as probability of
predicted knowledge units pairs over the actual number of knowledge unit pairs and
 {\it recall} is the percentage of knowledge unit pairs correctly classified. Mathematically,
  {\it precision}, {\it recall}and {\it F1-score} of 
the learner can be denoted as follows:

{\[
\begin{array}{ll}
prec_j &= precision_j = \frac{n_{jj}}{\sum_{i}n_{ij}}\\
pd_j &= recall_j = \frac{n_{jj}}{\sum_{i}n_{ji}}\\ 
F1_{j} &= 2*pd_j*prec_j/(pd_j + prec_j)
\end{array}
\]}
Where ${\sum_{i}n_{ji}}$ is the actual number of knowledge units in class $C_j$
and $\sum_{i}n_{ij}$ is the predicted number of knowledge units in class $C_j$.


Recall from Algorithm~1 that we call differential evolution once for each
optimization goal. Generally, this goal depends on which metric is most important for
the business case. In this work, we use $F1$ as the score because it controls
the tradeoff between precision and recall, which is also consistent with Xu et al.\cite{xu2016predicting}
and is also widely used in software engineering
community to evaluate classification results\cite{wang2016automatically,menzies2007data,fu2016tuning,kim2008classifying}.


\begin{table}[!htp]
\centering
\caption{Comparison of Our Baseline Method with Xu et al.'s}
\resizebox{0.48\textwidth}{!}{
\begin{tabular} {@{}l l  c c  c c c@{}}
\hline
   \multirow{2}{*}{Metrics} &  \multirow{2}{*}{Methods} &  \multirow{2}{*}{Duplicate} &  
   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Direct \\ Link\end{tabular}} &
   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Indirect \\ Link\end{tabular}} & 
   \multirow{2}{*}{Isolated} &  \multirow{2}{*}{Overall} \\ \\ \hline
   \multirow{2}{*}{Precision}
    & Our SVM &\textbf{0.724} &0.514 & 0.779 &0.601& 0.655 \\
   & Xu's SVM &0.611 &\textbf{0.560} &\textbf{0.787}&\textbf{0.676}&\textbf{0.659} \\ \hline
   \multirow{2}{*}{Recall} 
   & Our SVM & 0.525& \textbf{0.492} & 0.970 & \textbf{0.645}  & 0.658 \\ 
   & Xu's SVM  & \textbf{0.725} &0.433    &\textbf{0.980}  & 0.538 &\textbf{0.669}  \\ \hline
   \multirow{2}{*}{F1-score}
   & Our SVM & 0.609 &  \textbf{0.503} &0.864  &  \textbf{0.622}&0.650 \\ 
   & Xu's SVM & \textbf{0.663} &  0.488  & \textbf{0.873} &  0.600 &\textbf{0.656} \\ \hline
   \multirow{2}{*}{Accuracy} 
   &  Our SVM&0.525  &  0.493 & 0.970 & 0.645 &0.658 \\
   & Xu's SVM & - &  -  &- &  - &\textbf{0.669} \\ \hline
 \end{tabular}}
\label{tab:baseline}
\end{table}

\begin{figure}[!htp]
    \centering
     \includegraphics[width=0.49\textwidth]{pic/OurSVM-Xu'sSVM.pdf} % THIS IS FOR SINGLE COLUM
    \caption{Score Delta between Our SVM with Xu's SVM in ~\cite{xu2016predicting} in terms of Precision, Recall and F1-score.}
    \label{fig:OurSVM-Xu'sSVM}
\end{figure}

\section{Results}
In this section, we present our experimental results. To answer research questions raised in
Section~\ref{RQ}, we conducted two experiments:(1) comparing Xu's wordembedding+SVM with Ours wordembedding+SVM (2) comparing Tuning SVM method with CNN. Since we used the same training and testing data set provide by Xu
et al.\cite{xu2016predicting} and conducted our experiment in the same procedure and metrics, we simply used the results reported in the work by Xu et al.\cite{xu2016predicting} for performance comparison. 

%%%% baseline comparision %%%%%%
%\begin{table}[!htp]
%%\renewcommand{\baselinestretch}{0.35}
%%\scriptsize
%\centering
%  \caption{Comparison of our baseline method with Xu et al's }
%\resizebox{0.47\textwidth}{!}{
%  \begin{tabular} {@{}r|c c|c c|c c|c c@{}}
%   & \multicolumn{2}{c|}{Precision}  & \multicolumn{2}{c|}{Recall} &  \multicolumn{2}{c|}{F1-score} & \multicolumn{2}{c}{Accuracy}\\
%    \cline{2-9}
%    & Ours  &  \cite{xu2016predicting} & Ours  &  \cite{xu2016predicting}  &  Ours  &\cite{xu2016predicting} & Ours  &\cite{xu2016predicting}\\
%    \hline
%    Duplicate & 0.736  &0.611 & 0.550&0.725  &0.629&  0.663  &0.550&  -\\
%    Direct Link & 0.521 &0.560 & 0.502& 0.433  &0.511&  0.488& 0.402&  - \\
%    Indirect Link & 0.793  &0.787& 0.970&0.980  &0.873& 0.873  &0.970&  -\\
%    Isolated & 0.606  &0.676& 0.645 &0.538   &0.625&  0.600 & 0.645&  -\\
%    Overall & 0.664  &0.658&0.667&0.669   &0.660&  0.656 & 0.667&  0.669\\
%  \end{tabular}}
%\label{tab:baseline}
%\end{table}

\textbf{RQ1: Can we reproduce Xu et al.'s baseline results (wordembedding+SVM)?}



To answer this question, we strictly follow Xu et al.'s procedure\cite{xu2016predicting}. We 
use the SVM learner from scikit-learn setting $\gamma = \frac{1}{200}$ and $kernel= $``rbf''. After that, the same training and testing knowledge unit pairs are applied.

 \tab{baseline}  and \fig{OurSVM-Xu'sSVM} show the performance scores and corresponding score delta between our baseline method with
 Xu et al.'s in terms of accuracy, precision, recall and F1-score. As we can see, 
 when predicting these four different types of relatedness between knowledge unit pairs,
 our WordEmbedding+SVM method has  very  similar performance scores to the baseline method
 reported by Xu et al in ~\cite{xu2016predicting}, with the maximum difference less than $0.2$.  
 Except for {\it Duplicate} type, where our baseline 
has a higher {\it precision} (i.e., $0.724$ v.s. $0.611$) but a lower {\it recall} (i.e., $0.525$ v.s.$0.725$). \fig{OurSVM-Xu'sSVM} shows the score delta between our implementation
and Xu's implementation of WordEmbedding+SVM in terms of {\it precision}, {\it recall}
and {\it F1-score}. Any bar above zero means that our implementation has a better performance score than Xu's on predicting that specific KU relatedness class. As we can see, most 
of the differences ($\frac{8}{12}$) are within 0.05 and overall performance score delta shows that
our implementation is a slightly worse than Xu's.


This validation  is  very important to our work since, without the original tool released by Xu et al,
we want to make sure that our reimplementation of their baseline method (WordEmbedding + SVM)does not have much difference
than theirs, which make the following study more sensible. Otherwise, it will introduce many
biases into this study.

\vskip 1ex
 \begin{myshadowbox}
Overall, our reimplementation of WordEmbedding + SVM
has very closed performance in all the evaluated metrics 
 compared to the baseline method reported in \cite{xu2016predicting}.
Therefore, we believe our reimplementation can be treated as the baseline method in the following
experiment.
 \end{myshadowbox}


\begin{table}[!htp]
\centering
\caption{Comparison of Tuned SVM with Xu's CNN method. }
\resizebox{0.48\textwidth}{!}{
\begin{tabular} {@{}l l  c c  c c c@{}}
\hline
   \multirow{2}{*}{Metrics} &  \multirow{2}{*}{Methods} &  \multirow{2}{*}{Duplicate} &  
   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Direct \\ Link\end{tabular}} &
   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Indirect \\ Link\end{tabular}} & 
   \multirow{2}{*}{Isolated} &  \multirow{2}{*}{Overall} \\ \\ \hline
  \multirow{3}{*}{Precision} 
 % & Our SVM &0.736 &0.521 & 0.793 &0.606& 0.664 \\
   & Xu's SVM&0.611 &0.560 &0.787&0.676&0.658 \\ 
   & Xu's CNN&\textbf{0.898} & 0.758&0.840 &0.890 &0.847 \\
   & Tuned SVM&0.885 & \textbf{0.851}&\textbf{0.944} &\textbf{0.903} &\textbf{0.896}\\ \hline
   \multirow{3}{*}{Recall} 
   %& Our SVM & 0.550& 0.502 & 0.970 & 0.645  & 0.667 \\ 
   & Xu's SVM& 0.725 &0.433    &0.980  & 0.538 &0.669  \\
   & Xu's CNN& \textbf{0.898}&\textbf{0.903}    &0.773  & 0.793 &0.842  \\ 
   & Tuned SVM& 0.860 &0.828    &\textbf{0.995}  & \textbf{0.905} &\textbf{0.897}  \\  \hline
   \multirow{3}{*}{F1-score}
   %&Our SVM & 0.629 &  0.511 &0.873  &  0.625&0.660 \\ 
   & Xu's SVM& 0.663 &  0.488  & 0.873 &  0.600 &0.656 \\ 
   & Xu's CNN& \textbf{0.898} &  0.824  & 0.805 &  0.849 &0.841 \\
   & Tuned SVM& 0.878 & \textbf{ 0.841}  &\textbf{ 0.969} &  \textbf{0.909} &\textbf{0.899} \\\hline
%   \multirow{2}{*}{Accuracy} &  Our SVM&0.550  &  0.402 & 0.970 & 0.645 &0.667 \\
 %  & Xu's SVM& - &  -  &- &  - &0.669 \\ \hline
 \end{tabular}}
\label{tab:RQ2}
\end{table}

 \begin{figure}[!htp]
    \centering
    \includegraphics[width=0.49\textwidth]{pic/TunedSVM-CNN.pdf} % THIS IS FOR SINGLE COLUM
    \caption{Score Delta between Tuned SVM and CNN method in terms of Precision, Recall and F1-score.}
    \label{fig:TunedSVM-CNN}
\end{figure}

 \begin{figure}[!htp]
    \centering
%    \includegraphics[width=0.48\textwidth,height=2.7in]{pic/workflow.pdf} % THIS IS FOR SINGLE COLUM
     \includegraphics[width=0.49\textwidth]{pic/TunedSVM-SVM.pdf} % THIS IS FOR SINGLE COLUM
    \caption{Score Delta between Tuned SVM and Xu's SVM in terms of Precision, Recall and F1-score.}
    \label{fig:TunedSVM-SVM}
\end{figure}
\textbf{RQ2: Is tuning SVM comparable with Xu et al.'s deep learning method in terms of performance scores?}

To answer this question, we run the experiment designed as~\fig{workflow}, where DE is applied to 
find the optimal parameters for SVM based on the training and tuning data. Then the optimal tunings
applied on the SVM model and evaluate the built learner on testing data. Note that, in this study,
since we mainly focus on {\it precision}, {\it recall} and {\it F1-score} measures and {\it F1-score} is the
harmonic mean of {\it precision} and {\it recall}, we use {\it F1-score} as the tuning goal for DE.
In other words, when tuning parameters,  DE expects to find a pair of candidate parameters that maximize
{\it F1-score}. For CNN method, we simply adopt the Xu et al's results in ~\cite{xu2016predicting} because(1)deep learning method is not as easy as implementing SVM method, where a little difference in the structure might lead to significant result;(2) we are evaluating the learners with the same measures and use
the same training and testing data.


\tab{RQ2} presents the performance scores of  Xu et al's baseline, Xu et al's CNN method and Tuned SVM 
for all metrics, where the highest score for each type of KU pair
relatedness is marked in bold.  Without tuning, Xu et al's CNN method outperform
the baseline SVM in $\frac{10}{12}$ evaluation metrics across all 4 classes. 
The largest improvement is $0.47$ for {\it recall} on {\it Direct Link} class. However, after tuning SVM, the deep learning method does  not have such advantage. Specifically, \fig{TunedSVM-CNN} shows in detail the score delta between tuned SVM and CNN method. Any bar above indicates that tuned SVM has a better performance score than CNN.
According to \fig{TunedSVM-CNN}, CNN only has a slightly better performance on {\it Duplicate} class for {\it precision},{\it recall} and {\it F1-score} and a higher {\it recall} on {\it Direct link} class. However,
the largest differences is less than $0.1$. On the other hand, for most cases($\frac{8}{12}$ evaluation scores across four classes), Tuned SVM has better performance scores than CNN, where the largest advantage is $0.222$ and that's
$28\%$ improvements over CNN on predicting {\it Indirect Link} for the {\it recall} measure.

In \fig{TunedSVM-SVM}, we compare the performance delta of tuned SVM with Xu's untuned SVM. First of all,
tuning does not degrade SVM's performance, instead, it improves all performance scores of all metrics on predicting four different KU relatedness classes.
Furthermore, tuning dramatically improves scores on predicting some classes of KU relatedness. For example, the {\it recall} of predicting {\it Direct\_link} is increased from $0.433$ to $0.903$, which is $108\%$ improvement over Xu's untuned SVM(To be fair for Xu et al, it is still $84\%$ improvement over our untuned SVM). At the same time, the corresponding {\it precision} and {\it F1} scores of predicting {\it Direct\_Link} are increased from $0.560$ to $0.851$ and $0.488$ to $0.841$,  which are $52\%$ and $72\%$ improvement over Xu's original report, respectively.
The similar pattern can also be observed in {\it Isolated} class. Overall, tuning helps improve the performance
of SVM by $0.238$, $0.228$ and $0.227$ in terms of {\it precision}, {\it recall} and {\it F1-score}
for all four KU relatedness classes, respectively.



 Based on the performance scores in \tab{RQ2} and score delta in \fig{TunedSVM-CNN} and \fig{TunedSVM-SVM},
 we can see that: (1) parameter tuning can dramatically improve the performance of WordEmbedding+SVM(the baseline method) for the multi-class KU relatedness prediction task;
 (2) With the optimal tunings, the traditional machine learning method, SVM, if not better, is at least comparable 
 with deep learning method, CNN, in terms of all evaluation metrics considered in this study. Note that we are not saying tuning SVM is significantly
 better than deep learning method since it can not be concluded from this experiment without further statistical analysis.
 Due the lack of evaluation score distributions of the CNN method in~\cite{xu2016predicting}, comparing a single value with our 10 repeats run results doesn't make sense. Therefore, this question will be investigated in the future work.
 
 The experimental results and our analysis indicate that
 
 
\vskip 1ex
 \begin{myshadowbox}
Tuning help improve the performance of the SVM baseline method for knowledge units relatedness prediction.
On average, the deep learning method, CNN, does not have any advantage over tuning baseline method in terms of all evaluation metrics.
 \end{myshadowbox}
 

\textbf{ RQ3: Is tuning SVM running faster than Xu et al.'s deep learning method in terms of runtime?}

The powerful capability of deep learning method
 is coming from the latest development of in computing system. As pointed by~\cite{le2013building} and its reference,
it is quite time and resource intensive to train deep learning algorithms to yield state of the art results.
For example, Le used  a cluster with 1,000 machines (16,000 cores) for three days to train a deep learning network on
a 10 million data set. Therefore, time and resource cost for deep learning method should be carefully
examined. Xu et al. also notice the importance of this issue.

When comparing the runtime of two learning methods, it obviously should be conducted under the
 same hardware settings. 
 Since we adopt the CNN evaluation scores from ~\cite{xu2016predicting},
 we can not run on our tuning SVM experiment under the exactly same system settings.
 However, our experiment is running under a system which is similar to the one used in \cite{xu2016predicting}.
 To allow readers to have a objective comparison, we provide the experimental environment as shown in~\tab{env}. Therefore, such comparison still can provide some insights. 
 To compare the runtime of Tuning SVM and CNN method, we record the start time and end time
 of the program execution, including parameter tuning, training model and testing model,
 to get the runtime of Tuning SVM method. 
 
 \begin{table}[!htp]
\centering
\caption{Comparison of Experimental Environment  }
% \resizebox{0.48\textwidth}{!}{
\begin{tabular} {@{}l  l l l @{}}
\hline
Methods&OS&CPU&RAM \\ \hline
Tuning SVM & MacOS10.12 & Intel Core i5 2.7 GHz & 8GB  \\
CNN& Windows7 &Intel Core i7 2.5 GHz & 16GB \\
\hline
\end{tabular}
% }
\label{tab:env}
\end{table}

According to Xu et al, it took {\it $14$ hours} for them to train the  the CNN model
to get a relative low loss convergence($< e^{-3}$)~\cite{xu2016predicting}.
However, in this experiment, it only takes takes {\it $10$ minutes} to run SVM with parameter tuning
by DE on a similar environment shown in \tab{env} to finish 
the whole experiment, which includs parameter tuning, training model and prediction,
Therefore, the simple parameter tuning method on SVM is $80X$ faster than  Xu's deep learning method.
% \wei{Note that the runtime shown in \tab{env} does not take account of training word2vec model.
%  The reason that we didn't consider it is that both tuning SVM method and CNN method require word embedding as input.}
 
\vskip 1ex
 \begin{myshadowbox}
Compared to CNN method, tuning SVM is about $80X$ faster in terms of model building.
Therefore, deep learning method does not have any advantage over tuning method in terms
of resource cost, especially when the model has to be updated periodically. 
 \end{myshadowbox}
 
 
 
\section{Threads to Validity}
For any empirical study, there might be some potential threats that
affect the conclusion. We identify the following threats to validity:

Threats to \textbf{internal validity} refers to consistency of the results 
obtained from the result. In our study,  to study how
tuning can improve the performance of baseline methods and how well
it perform compared with deep learning method, we select
Xu et al's  WordEmbedding+SVM baseline method as a case study. Since the original implementation of 
WordEmbedding+SVM (baseline 2 method in \cite{xu2016predicting}) is not 
publicly available, we have to reimplement our version of WordEmbedding+SVM as
the baseline method in this study. As shown in RQ1, our implementation has
quite similar results to Xu et al.'s on the same datasets. The variance might 
result from the word2vec that is trained from 100,000 randomly selected data.
Hence, we believe that our implementation reflect the original
 baseline method in \cite{xu2016predicting}.
 
 Threats to \textbf{external validity} represent if the results are of relevance for
 other cases, or the ability to generalize the observations in a study. In this study,
 we compare our tuning baseline method with deep learning method in terms of
 precision, recall, F1-score and accuracy. The experimental results are quite consistent
 for this knowledge units relatedness prediction tasks, i.e.,  muti-classification problem, in text mining. 
 Nonetheless, we do not claim that our findings can be generalized to all software analytics tasks. 
 However, those other software analytics tasks often apply deep learning
 methods~\cite{choetkiertikul2016deep, wang2016automatically} and compare with
 the same regular learners explored in this paper, so it is quite possible that
  the conclusions of this paper apply to other software engineering tasks.
 
 


\section{Conclusion}

In this paper, we perform an comparative study to investigate
how tuning can improve the baseline method compared with
the state-of-art deep learning method, CNN, for predicting
knowledge units relatedness on Stack Overflow. Our experimental
results show that:

\bi
\item Tuning improves the performance of baseline methods. 
At least for WordEmbedding+SVM(baseline in \cite{xu2016predicting}) method, if not better,
it performs as well as the proposed CNN method in \cite{xu2016predicting}.
\item The baseline method with parameter tuning runs much faster than complicated deep learning.
In this study, tuning SVM runs $80X$ faster than CNN method.
\ei


Our findings have important implications for the software engineering community.
Although deep learning methods have widely and successfully applied in other community
to address problems, like pattern recognition, machine translation and recommendation
systems, it is still unclear which type of SE task is best fit for deep learning. There are lots
of possibilities in SE field to explore the application of deep learning. However, as pointed out
at the beginning of this paper, deep learning is so resource-consuming technique
and SE analytics task is usually light-weight,  {\it the  tradeoff between performance and cost should never
be ignored when applying deep learning}. The effort spent on deep learning might not be worthy the performance
improved by deep learning. As shown in our experimental results, our tuning SVM gets the similar,
 or even better, performance to deep learning while $80X$ faster.
Therefore, it should not happen any more that simply apply deep learning on SE tasks
without reporting and analyzing the resource consumption as well as the tradeoff.  

Based on the results of this study, we recommend that before applying 
deep learning method on SE tasks, consider simple techniques to improve the baseline method.
Simple methods do matter. For example, tuning parameters dramatically improve the 
performance of SVM method.  Also, Grid search should
never be used as {\it de facto} tuning method according to \cite{fu2016differential,bergstra2012random}. 
Xu et al failed to find the best tunings by using grid search for
the baseline method might confirm this point.

As to the future work, we will explore more simple techniques to solve SE tasks and also
investigate how deep learning techniques could be applied effectively in software engineering
field. 




\bibliographystyle{ACM-Reference-Format}
\bibliography{0.main} 

\end{document}
