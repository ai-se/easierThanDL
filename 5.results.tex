
%%%% baseline comparision %%%%%%
\begin{table}[!htp]
%\renewcommand{\baselinestretch}{0.5}
%\scriptsize
\centering
  \begin{tabu} to 0.25\textwidth{r|c c|c c|c c}
   & \multicolumn{2}{c|}{Precision}  & \multicolumn{2}{c|}{Recall} &  \multicolumn{2}{c}{F1-score} \\
    \cline{2-7}
    & Ours  &  \cite{xu2016predicting} & Ours  &  \cite{xu2016predicting}  &  Ours  &\cite{xu2016predicting} \\
    \hline
    Duplicate & 0.736  &0.611 & 0.550&0.725  &0.629&  0.663  \\
    Direct Link & 0.521 &0.560 & 0.502& 0.433  &0.511&  0.488  \\
    Indirect Link & 0.793  &0.787& 0.970&0.980  &0.873& 0.873  \\
    Isolated & 0.606  &0.676& 0.645 &0.538   &0.625&  0.600  \\
    Overall & 0.664  &0.658&0.667&0.669   &0.660&  0.656\\
  \end{tabu}
  \caption{Comparison of our reimplementation of WordEmbedding + SVM with Xu et al's
   in terms of precision, recall, F1-score and averaged }
\label{tab:baseline}
\end{table}

\section{Results}
In this section, we present our experimental results. Since we use the same data set provide by Xu
et al.\cite{xu2016predicting} and conducted our experiment in the same procedure and metics. 
Therefore, we used the results reported in the work by Xu et al.\cite{xu2016predicting} for performance
comparison. We compare the performance of tuned SVM with the state-of-art CNN method and answer
our research questions proposed in Section~\ref{RQ}.


\textbf{RQ1: Can we reproduce Xu et al.'s baseline results (wordembedding+SVM)?}

To answer this question, we strictly follow Xu et al.'s procedure\cite{xu2016predicting}. We 
use the SVM from scikit-learn with $\gamma = 1/200$ and $kernel= ``rbf''$. After that,
the same training and testing knowledge unit pairs are applied.

 \tab{baseline} presents the precision, recall and F1-score for each class and 
 the overall values, respectively. As we can see, 
 when predicting these four different types of relatedness between knowledge unit pairs,
 our WordEmbedding+SVM method has  a very closed performance compared with Xu et al.'s
, with the difference less than $0.1$.  Except for {\it Duplicate} type, where our method 
has a higher {\it precision} (i.e., $0.736$ v.s. $0.611$) but a lower {\it recall} (i.e., $0.550$ v.s.$0.725$).
However, our read of the averaged values across four classes is that both methods have
a very small difference.

This validation  is  very important to our work since, without the original tool released by Xu et al,
we want to make sure that our reimplementation of their baseline method does not have much difference
than theirs, which make the following investigation more sensible.

\begin{lesson}
Overall, our reimplementation of WordEmbedding + SVM
has very closed performance in all the evaluated metrics 
 compared to the baseline method reported in \cite{xu2016predicting}.
Therefore, our reimplementation can be treated as the baseline method in the following
experiment.
\end{lesson}